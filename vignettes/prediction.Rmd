---
title: "Predicting_Refugee_Population_with_Gravity_Model"
date: '`r Sys.Date()`'
output: 
  rmarkdown::html_vignette:
    toc: yes
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Predicting_Refugee_Population_with_Gravity_Model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

To support programme development, UNHCR operations have to develop population planning figures. This is usually done through expert judgement. Though a predictive model, based historical data and pull/push indicators, can offer the advantage of cross checking expert appreciation.

The model presented in this tutorial is a [gravity model of migration](https://en.wikipedia.org/wiki/Gravity_model_of_migration), named because of the analogy with the Newtonian theory of gravitation, which assumes the movement between two countries is proportional to their size (population or GDP) and inversely proportional to the physical distance between them. It has long been popular for analyzing economic phenomena related to the movement of goods, service, capital or even people due to the extraordinary stability and its power to explain bilateral flows. 

With the collected data from 2000 to 2017, we predicted the refugee flows from any country of origin to any country of asylum in 2018 and 2019.

All data used in this tutorial are public open data:

- [UNHCR refugee statistics](https://www.unhcr.org/refugee-statistics/)

- [GDP and Population data World Bank](https://data.worldbank.org/)

- [Complementary GDP data ](https://countryeconomy.com/)

- [Gravity related data CEPII (Centre d'Etudes Prospectives et d'Informations Internationales)](http://www.cepii.fr/CEPII/en/bdd_modele/presentation.asp?id=8)

- [Political stability and absence of violence World Bank](https://datacatalog.worldbank.org/political-stability-and-absence-violenceterrorism-estimate)

Other potential data sources, like the [ACLED - Armed Conflict Location & Event Data Project](https://acleddata.com), the [Uppsala Conflict Data Program](https://ucdp.uu.se/), [INFORM Global Risk Index](https://drmkc.jrc.ec.europa.eu/inform-index)) were initially tested but not included in the model due to  lack of data; bad performance in the modelling or hard to do the prediction.



```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
## Getting all necessary package
using <- function(...) {
  libs <- unlist(list(...))
  req <- unlist(lapply(libs,require,character.only = TRUE))
  need <- libs[req == FALSE]
  if (length(need) > 0) {
    install.packages(need, repos = 'https://mirrors.tuna.tsinghua.edu.cn/CRAN/')
    lapply(need,require,character.only = TRUE)
  }
}

using(
  ## Data
  'wbstats',
  'readxl',
  'data.table',
  'DT',
  'stargazer',
  'haven',
  "httr", "jsonlite",
  ## Mapping
  'ISOcodes',
  'forecast',
  'broom',
  'Metrics',
  'GGally','ggfortify','tseries','lmtest',# model diagnosis
  #plot
  'R.utils',
  'png',
  'grid',
  'ggplot2',
  'gganimate',
  'gghighlight',
  'ggpubr',
  'ggalt',
  'scales', # Scale Functions for Visualization
  'networkD3',
  'magrittr',
        
  # manipulation'tidyverse',
  'dplyr',
  'tidyr',
  'reshape2',
  'zoo',
  'xml2',
  'tidyverse',
  'broom',
  'forcats', # Tools for Working with Categorical Variables
  #time series
  'plm', # Linear Models for Panel Data
  'texreg'
)

unhcr_style <- function() {
  font <- "Lato"
  ggplot2::theme(

    #This sets the font, size, type and colour of text for the chart's title
    plot.title = ggplot2::element_text(family = font, size = 20, face = "bold", color = "#222222"),

    #This sets the font, size, type and colour of text for the chart's subtitle,  as well as setting a margin between the title and the subtitle
    plot.subtitle = ggplot2::element_text(family = font, size = 16, margin = ggplot2::margin(9,0,9,0)),
    plot.caption = ggplot2::element_blank(),

    #This sets the position and alignment of the legend, removes a title and backround for it and sets the requirements for any text within the legend. The legend may often need some more manual tweaking when it comes to its exact position based on the plot coordinates.
    legend.position = "top",
    legend.text.align = 0,
    legend.background = ggplot2::element_blank(),
    legend.title = ggplot2::element_blank(),
    legend.key = ggplot2::element_blank(),
    legend.text = ggplot2::element_text(family = font, size = 13, color = "#222222"),

    #This sets the text font, size and colour for the axis test, as well as setting the margins and removes lines and ticks. In some cases, axis lines and axis ticks are things we would want to have in the chart
    axis.title = ggplot2::element_blank(),
    axis.text = ggplot2::element_text(family = font, size = 13, color = "#222222"),
    axis.text.x = ggplot2::element_text(margin = ggplot2::margin(5, b = 10)),
    axis.ticks = ggplot2::element_blank(),
    axis.line = ggplot2::element_blank(),

    #This removes all minor gridlines and adds major y gridlines. In many cases you will want to change this to remove y gridlines and add x gridlines.
    panel.grid.minor = ggplot2::element_blank(),
    panel.grid.major.y = ggplot2::element_line(color = "#cbcbcb"),
    panel.grid.major.x = ggplot2::element_blank(),

    #This sets the panel background as blank, removing the standard grey ggplot background colour from the plot
    panel.background = ggplot2::element_blank(),

    #This sets the panel background for facet-wrapped plots to white, removing the standard grey ggplot background colour and sets the title size of the facet-wrap title to font size 22
    strip.background = ggplot2::element_rect(fill = "white"),
    strip.text = ggplot2::element_text(size  = 13,  hjust = 0)
  )
}

#Left align text
left_align <- function(plot_name, pieces){
  grob <- ggplot2::ggplotGrob(plot_name)
  n <- length(pieces)
  grob$layout$l[grob$layout$name %in% pieces] <- 2
  return(grob)
}


## a little help function to better format numbers
format_si <- function(...) {
  function(x) {
    limits <- c(1e-24, 1e-21, 1e-18, 1e-15, 1e-12,
                1e-9,  1e-6,  1e-3,  1e0,   1e3,
                1e6,   1e9,   1e12,  1e15,  1e18,
                1e21,  1e24)
    prefix <- c("y",   "z",   "a",   "f",   "p",
                "n",   "",   "m",   " ",   "k",
                "M",   "G",   "T",   "P",   "E",
                "Z",   "Y")

    # Vector with array indices according to position in intervals
    i <- findInterval(abs(x), limits)

    # Set prefix to " " for very small values < 1e-24
    i <- ifelse(i == 0, which(limits == 1e0), i)

    paste(format(round(x/limits[i], 1),
                 trim = TRUE, scientific = FALSE, ...),
          prefix[i])
  }
}
```

## Dataset used for Analysis 

The dataset includes the refugee populations from COO to COA across time, as well as push and pull factors like GDP, Population, Political Stability. Additionally, the generalized distance between the COO and COA consists of Geo-distance, if the pair of countries have contiguous border, do they have colonial relationship in history, do they use the same language, do they use common currency, do they have the same religious belief et al. 

###  UNHCR Dataset

The whole dataset is Panel data (also known as longitudinal or cross-sectional time-series data). The behavior of individual are observed across time. In our case,

 * Individual: country pair of COO and COA, 
 * COO:  Country of Origin, 
 * COA:  Country of Asylum, 
 * Time: year. 



```{r refugee, echo=TRUE, message=FALSE, warning=FALSE}
#  download the REFUGEE DATA

time_series <-  unhcrdatapackage::end_year_population_totals

reference <- unhcrdatapackage::reference

ref_data <- end_year_population_totals[,c(1:6)]

colnames(ref_data) <- c("year","iso_o","iso_d","origin","asylum","refugee")

ref_data[is.na(ref_data)] <- 0

## Get the bilateral dataset and exclude the origin/destination as "Various/Unknown" and "Stateless"

ref_bidata <- ref_data %>% 
  filter(iso_o != "UKN " & iso_o != "STA "& iso_d != "UKN" )

## generante the country pair to get the unique ID

ref_bidata$pair <- paste(ref_bidata$iso_o, ref_bidata$iso_d, sep = "")

## rearrange the dataset with country pair ID and year, with the 'plm' package

ref_bidata <- pdata.frame(ref_bidata, index = c("pair","year"))

## convert the column year into integer, or numeric

ref_bidata$year <- as.numeric(as.character(ref_bidata$year))

## convert the columns 'origin''iso_o' 'asylum' 'iso_d' into character

fun.char <- function(x) {as.character(x)}
ref_bidata[,c(2:5,7)] <- apply(ref_bidata[,c(2:5,7)],2,fun.char)

rownames(ref_bidata) <- NULL

# filter the data with year, we would like to start with 2000, which requires the lag data in 1999
# omit the pair when asylum and oirgin are the same

ref_bidata <- ref_bidata %>% filter(year>1998) %>% filter(iso_o != iso_d)


```

### World bank data

Political Stability and Absence of Violence/Terrorism: Estimate - Political Stability and Absence of Violence/Terrorism measures perceptions of the likelihood of political instability and/or politically-motivated violence, including terrorism. Estimate gives the country's score on the aggregate indicator, in units of a standard normal distribution, i.e. ranging from approximately -2.5 to 2.5.

 The Worldwide Governance Indicators (WGI) project reports aggregate and individual governance indicators for over 200 countries and territories over the period 1996–, for six dimensions of governance:

 *  Voice and Accountability
 *  Political Stability and Absence of Violence
 *  Government Effectiveness
 *  Regulatory Quality
 *  Rule of Law
 *  Control of Corruption
 

```{r echo=TRUE, message=FALSE, warning=FALSE}
#wb_data DATA
wb_data <- wbstats::wb( indicator = c("SP.POP.TOTL", ## Population
                             "NY.GDP.MKTP.CD",  ## GDP
                             "NY.GDP.PCAP.CD", ## GDP per capita
                             "NY.GNP.PCAP.CD", ## GNP per capita
                             "SI.POV.GINI", ## Gini Index
                             "PV.EST", # Political Stability and Absence of Violence/Terrorism
                             "VA.EST" , #    Voice and Accountability
                             "GE.EST" , # Government Effectiveness
                             "RQ.EST" , # Regulatory Quality
                             "RL.EST" , # Rule of Law
                             "CC.EST" #, # Control of Corruption
                             # "DT.ODA.DACD.AGPA.OCOM.CD", # Gross ODA aid disbursement for other commodity assistance, DAC donors total
                             # "AG.LND.PRCP.MM"
    ),
    startdate = 2000, enddate = 2019, return_wide = TRUE)

# Renaming variables for further matching
names(wb_data)[1] <- "iso"
names(wb_data)[2] <- "year"
wb_data$year <- as.integer(wb_data$year)
wb_data <-  wb_data %>%
            select(iso,year,SP.POP.TOTL,NY.GDP.MKTP.CD,PV.EST) %>%
            dplyr::rename(pop = SP.POP.TOTL, gdp = NY.GDP.MKTP.CD, pv = PV.EST)

## The data

```
The GDP data in many countries/years (i.e. Syria, North Korea, Somalia, Eritrea et al.) are missing in WB data, to solve which we need to bring in other data resources. Some big data gaps are filled manually by data from [country Economy website](https://countryeconomy.com/).  Then I use the nearest exsiting data to replace the NA data.


```{r echo=TRUE, message=FALSE, warning=FALSE}

# Syria GDP, the GDP data are saved in a csv file 
SYR <- read.csv("data-raw/SYR_GDP.csv")
SYR <- SYR[,-1]
YEAR <- c(2000:2018)
for (i in YEAR) {
  wb_data[which(wb_data$iso == "SYR" & wb_data$year == i),"gdp"] <- SYR[which(SYR$iso == "SYR" & SYR$year == i),"gdp"]
}

# North Korea GDP
PRK <- read.csv("data-raw/North_korea_gdp.csv")
PRK <- PRK[,-1]
for (i in YEAR) {
  wb_data[which(wb_data$iso == "PRK" & wb_data$year == i),"gdp"] <- PRK[which(PRK$iso == "PRK" & PRK$year == i),"gdp"]
}

# Somalia, or you can input the gdp data as an assay

wb_data[which(wb_data$iso == "SOM"), 'gdp'] <- c(2052e6, # 2000   #
                                                 1303e6, # 2001   #
                                                 1219e6, # 2002   #
                                                 1517e6, # 2003   #
                                                 1984e6, # 2004   #
                                                 2316e6, # 2005   #
                                                 2390e6, # 2006   #
                                                 2483e6, # 2007   #
                                                 2600e6, # 2008   #
                                                 1247e6, # 2009   #
                                                 1093e6, # 2010   #
                                                 3499e6, # 2011   #
                                                 3611e6, # 2012   #
                                                 3892e6, # 2013   #
                                                 3964e6, # 2014   #
                                                 4049e6, # 2015   #
                                                 4198e6, # 2016   #
                                                 4509e6, # 2017   #
                                                 4721e6, # 2018   #
                                                 NA) # 2019

wb_fill <- wb_data %>% group_by(iso) %>% 
  fill(gdp, .direction = "downup") %>% 
  ungroup()

wbdata_o <- wb_fill  %>% dplyr::rename(iso_o = iso, pop_o = pop, gdp_o = gdp,pv_o = pv)
wbdata_d <- wb_fill  %>% dplyr::rename(iso_d = iso, pop_d = pop, gdp_d = gdp, pv_d = pv)
```

### GeoDist, CEPII GRAVITY DATA

[GeoDist](http://www.cepii.fr/CEPII/fr/bdd_modele/presentation.asp?id=6) database, produced by CEPII (Centre d'Études Prospectives et d'Informations Internationales), provides several geographical variables, in particular bilateral distances measured using city-level data to account for the geographic distribution of population inside each nation. Different measures of bilateral distances are available for 225 countries. For most of them, different calculations of "intra-national distances" are also available.

Some fix are required to account for certain countries. we select data from 2014, assuming that this data is not chaning much over time.

```{r echo=TRUE, message=FALSE, warning=FALSE, , warning=FALSE, comment=NA}
# The Distance/If share contiguous border/Has ever Colonial/If use the same language, from 
url <- paste( 'http://www.cepii.fr/DATA_DOWNLOAD/gravity/gravdata_cepii.zip') 
destfile = "data-raw/gravdata_cepii.zip" 
if (!file.exists(destfile)) {
    #setInternet2(TRUE)
    download.file(url ,destfile,method = "auto")
    unzip(destfile,exdir = getwd())  
    }

gravdata.or <- read_dta("data-raw/gravdata.dta")
# levels(as.factor(gravdata.or$year))

grav_data <- gravdata.or  %>% 
             filter(year == "2014") %>% 
             select(iso3_o, iso3_d, contig, comlang_ethno, col45, distw, comcur, comrelig) %>% 
             dplyr::rename(iso_o = iso3_o,
                    iso_d = iso3_d,
                    comlang = comlang_ethno,
                    colony = col45)
```

The gravity data for some countries/territories needs to be corrected because of domian/name changing, such as South Sudan, Democratic Republic of the Congo (zaire), Romanian (ROM to ROU), Serbia (YUG to SRB) and Palestine. I managed the data for Palestine by connecting Geo variables for Israel and social-eco variables for Jordan.


```{r echo=TRUE, message=FALSE, warning=FALSE, , warning=FALSE, comment=NA}

# SSD
ssd <- grav_data %>% 
       filter(iso_o == "SSD")

## use the data of SDN(sudan) to replace that of SSD
SDN_o <- grav_data %>% 
         filter(iso_o == "SDN")

SDN_d <- grav_data %>% 
         filter(iso_d == "SDN")

SDN_o[,1] <- "SSD"
SDN_d[,2] <- "SSD"

grav_data_SSD <- rbind(SDN_o, SDN_d)
grav_data <- rbind(grav_data, grav_data_SSD)

grav_data[which(grav_data$iso_o == "SSD" & grav_data$iso_d == "SDN"),3] <- 1
grav_data[which(grav_data$iso_o == "SSD" & grav_data$iso_d == "SDN"),4] <- 1
grav_data[which(grav_data$iso_o == "SSD" & grav_data$iso_d == "SDN"),5] <- 1
grav_data[which(grav_data$iso_d == "SSD" & grav_data$iso_o == "SDN"),3] <- 1
grav_data[which(grav_data$iso_d == "SSD" & grav_data$iso_o == "SDN"),4] <- 1
grav_data[which(grav_data$iso_d == "SSD" & grav_data$iso_o == "SDN"),5] <- 1

## COD
COD_o <-  grav_data %>% 
          filter(iso_o == "ZAR") 

COD_d <-  grav_data %>% 
          filter(iso_d == "ZAR") 

COD_o[,1] <- "COD"
COD_d[,2] <- "COD"
grav_data_COD <- rbind(COD_o,COD_d)
grav_data <- rbind(grav_data,grav_data_COD)

## Romania
grav_data[which(grav_data$iso_o == "ROM"),"iso_o"] <- "ROU"
grav_data[which(grav_data$iso_d == "ROM"),"iso_d"] <- "ROU"

## Serbia

grav_data[which(grav_data$iso_o == "YUG"),"iso_o"] <- "SRB"
grav_data[which(grav_data$iso_d == "YUG"),"iso_d"] <- "SRB"

## PSE

israel <- grav_data %>% filter(iso_o == "ISR") %>% select(iso_d,distw,contig)
israel$contig[israel$iso_d == "ISR"] <- 1

Jordan <- grav_data %>% filter(iso_o == "JOR") %>% select(iso_d,comlang,colony,comcur,comrelig)
Jordan$comlang[Jordan$iso_d == "JOR"] <- 1

pse <- left_join(israel, Jordan, by = "iso_d")
pse$iso_o <-  "PSE"

## Assign values to the 'self-distance'

pse <- rbind(pse,c("PSE",1,1,1,0,1,1,"PSE"))

grav_data <- rbind(grav_data,pse)

grav_data$iso_d[grav_data$iso_o == "COD"&grav_data$iso_d == "ZAR"] <- "COD"

grav_data <- rbind(grav_data, c("SSD","SSD",1,1,0,1,1,1))

grav_data$contig[grav_data$iso_o == grav_data$iso_d] <- 1
grav_data$comlang[grav_data$iso_o == grav_data$iso_d] <- 1
grav_data$distw[grav_data$iso_o == grav_data$iso_d] <- 1
grav_data$comrelig[grav_data$iso_o == grav_data$iso_d] <- 1

fun.num <- function(x){as.numeric(x)}

grav_data[,3:8] <- apply(grav_data[,3:8],2,fun.num)

```

### Merging all data together

We can now merge all dataset.

Considering the value of some variables is relatively small (between 0 to 1), the variables with large values,like refugees, GDP, population, Geo-distance are transformed into log value for smoothing.

```{r echo=TRUE, message=FALSE, warning=FALSE}
## Joing with gravity data
refgrav <- left_join(ref_bidata, grav_data, by = c("iso_o","iso_d"))

## Joining with GDP & Political data for Origin
ref_grav_wb_o <- left_join(refgrav, wbdata_o, by = c("iso_o","year"))

## Joining with GDP & Political data for Destination
ref_grav <- left_join(ref_grav_wb_o, wbdata_d, by = c("iso_d","year"))

## Getting log value
ref_grav <- ref_grav %>%
            mutate(lref = log(refugee),
                     lndist = log(distw),
                     lpop_o = log(pop_o),
                     lpop_d = log(pop_d),
                     lgdp_o = log(gdp_o),
                     lgdp_d = log(gdp_d))

## convert the log(0) to NA

ref_grav$lref[ref_grav$lref == -Inf] <- NA

## check the duplicated ID-time

any(table(ref_grav$pair, ref_grav$year)>1)
```


## Modelling stage

### Dickey-Fuller test for stationary time series

Refugee populations are stock values. Ideally, the gravity model performs better with flow value. We could then compile the difference between the stock value would be one option. 

A Dickey-Fuller test can be used to check for stochastic trends and verify if the serie is stationary. A stationary series is one in which the properties – mean, variance and covariance, do not vary with time. For a series to be classified as stationary, it should not exhibit a trend.

If this is the case, and it is here, we can simply use the stock value (logarithm) to build the linear panel model with 'plm' package.

```{r echo=TRUE, message=FALSE, warning=FALSE, comment=""}
adf.test(ref_grav$refugee, k = 2)
```


### Panel Data Estimators

Lets build first model of random effect.

```{r echo=TRUE, message=FALSE, warning=FALSE, , comment=""}

random <- plm( lref ~
               lag(lref) + # lref - log of refugee
               lndist +    # lndist - log of distance
               contig +    # contig - contiguous border or not
               comlang +   # conlang - common language
               colony +    # colony - colonial relationship in history
               comcur +    # comcur - common currency
               comrelig +  # comrelig - common religion
               lpop_o +    # lpop_o - log of population on country of origin
               lpop_d +    # lpop_d - log of population on country of asylum
               lgdp_o +    # lgdp_o - log of GDP on country of origin
               lgdp_d +    # lgdp_d - log of GDP on country of asylum
               pv_o +      # pv_o - Political Stability and Absence of Violence/Terrorism for country of origin
               pv_d,        # pv_d - Political Stability and Absence of Violence/Terrorism for country of asylum

            data = ref_grav,
            index = c("pair","year"),
            model = "random")

summary(random)
```
The results show most of the variables are significant, except for the common currency and population of destination country. This suggests that they are not the determinant factors for the movement of refugees. 

If we look at coefficients, we notice that some variables (only significant ones) have __positive values__, such as  the refugees in last year, colonial relationship, common language, contiguous border, population of original country, GDP of destination country have positive impact on the refugee flows, while others, like Geo-distance, religion belief, GDP and stability of original country are __negative__ to the movement. Most of the coeffocents are in line with common sense, but still there are exceptions, like religion belief. The model reveals that the displaced people intend to move to the a country with different religion belief, which could be explained as many conflicts are religion related. 

We can also present the model through a more visual forrest plot

```{r echo=TRUE, message=FALSE, warning=FALSE}

random.tidy <- broom::tidy(random,
                                    conf.int = TRUE,
                                    conf.level = 0.95,
                                    exponentiate = TRUE
                                  )

dico <- as.data.frame(cbind(
  fullname <- c("lag(lref)",
                "lndist" ,
                "contig" ,
                "comlang" ,
                "colony" ,
                "comcur" ,
                "comrelig" ,
                "lpop_o" ,
                "lpop_d" ,
                "lgdp_o" ,
                "lgdp_d" ,
                "pv_o" ,
                "pv_d"),
  label <- c("Previous Refugee population",
                "Distance to country of Origin" ,
                "contiguous border or not" ,
                "common language" ,
                "colonial relationship in history" ,
                "common currency" ,
                "common religion" ,
                "population on country of origin" ,
                "population on country of asylum" ,
                "GDP on country of origin" ,
                "GDP on country of asylum" ,
                "Political Stability for country of origin" ,
                "Political Stability for country of asylum")))
names(dico)[1] <- "fullname"
names(dico)[2] <- "label"

#str(random.tidy)
random.tidy$fullname <- random.tidy$term
random.tidy <- merge(x = random.tidy, y = dico, by = "fullname", all.x = TRUE)
random.tidy$term <- random.tidy$label
random.tidy <- random.tidy[ !(is.na(random.tidy$term)), ]

random.tidy$term <- fct_reorder(random.tidy$term, random.tidy$estimate, min)

### Chart showing the model
#ggcoef(model.popgroup1a, 
plot1 <- ggcoef(random.tidy,        
       exponentiate = TRUE, color = "purple", shape = 18,  size = 3.5, exclude_intercept = TRUE, 
  vline_color = "red", vline_linetype =  "solid", errorbar_color = "black", errorbar_height = .25,
  conf.level = 0.95, conf.int = TRUE, 
  mapping = aes(x = estimate, y = term)) +
  xlab("Influence factor for each variable: Estimate of Odd Ratio") +
  ylab("") +
  labs(title = "Predictor for Refugee population growth",
           subtitle =  "Ordered by importance",
           caption = "The variable on the right side of the red line are predictors of increase, while those on the left are for decrease.\n The black lines (also called 'whiskers') around the point represent the confidence interval for each variable\n to be included in the model (the shorter the line, the more precise is the estimate)",
           x = NULL, y = NULL) +
    unhcr_style() +
      theme( plot.title = element_text(size = 13),
             plot.subtitle = element_text(size = 11),
             plot.caption = element_text(size = 7, hjust = 1),
            axis.text = element_text(size = 9),
            panel.grid.major.x = element_line(color = "#cbcbcb"), 
            panel.grid.major.y = element_blank(),
            strip.text.x = element_text(size = 11))

ggpubr::ggarrange(left_align(plot1, c("subtitle", "title", "caption")), ncol = 1, nrow = 1)

```

### Comparing models

There are two techniques have been widely used in In panel data anaylsis, fixed effects and random effects. To decide between fixed or random effects, we run a Hausman test where the null hypothesis is that the preferred model is random effects vs. the alternative the fixed effects (Green, 2008, chapter 9). The p-value is significant and indicates we should use fixed effect. However, an advantage of random effects is that you can include time-invariant variables (here distance, border, language, religion, and colonial relation). In the fixed effects model these variables are absorbed by the intercept. Another reason is the fixed effect will assign a coefficient for the time (year), which stands for some hidden factors not observable in that year. For training the model the results would be better. However, you cannot use it for the prediction, as you don't know the fixed effect in future.

```{r echo=TRUE, message=FALSE, warning=FALSE, comment=""}
# the fixed effects model
fix <- update(random, 
              model = "within")
# fix or random
phtest(fix, random)
```


[`plm`](https://cran.r-project.org/web/packages/plm/vignettes/plmPackage.html) provides various functions for estimation, like the first-difference model ("fd"), and the between model ("between"). The model estimation shows there is no great improvement for the random effect, so we are going to use the random model for prediction and accuracy test. 

```{r echo=TRUE, message=FALSE, warning=FALSE, comment=""}
#the between model
bw <- update(random, 
             model = "between")

#the pooling model 
pool <-  update(random, 
             model = "pooling")

# Amemiya estimator for random effect
tways <- update(random,
                effect = "twoways",
                model = "random",
                random.method = "walhus")


screenreg( list("random" = random,
                "pooling" = pool,
                "fix" = fix,
                "between" = bw,
                "two ways" = tways),
           digits = 5)
```

## Testing accuracy of prediction.

One of the most exciting part of modelling is to do the predictions. Here I divided the whole dataset into a training dataset and testing dataset to verify the model. However, the the lag function of plm does not work with the 'predict' funtion, instead of generating the lag values, it remains the same. Thus a new column for lag of refugees is added manually. 

```{r  echo=TRUE, message=FALSE, warning=FALSE, comment=""}
#a new column for lags

ref_grav$lag.lref <- lag(ref_grav$lref)

# the data for 2019 is not used here, as the Political Stability is missing in that year. This issue is solved in the long-term prediction chapter.

test.p <- ref_grav %>% 
          filter(year == 2018) %>% na.omit() 

train.p <- ref_grav %>% 
           filter(year < 2018)

# update the function with the new column, rather than using lag()
ran.p <- plm(lref ~ 
               lag.lref +  # lag.lref - log of refugee in last year
               lndist +    # lndist - log of distance
               contig +    # contig - contiguous border or not
               comlang +   # conlang - common language
               colony +    # colony - colonial relationship in history
               comcur +    # comcur - common currency
               comrelig +  # comrelig - common religion
               lpop_o +    # lpop_o - log of population on country of origin
               lpop_d +    # lpop_d - log of population on country of asylum
               lgdp_o +    # lgdp_o - log of GDP on country of origin
               lgdp_d +    # lgdp_d - log of GDP on country of asylum
               pv_o +      # pv_o - Political Stability and Absence of Violence/Terrorism for country of origin
               pv_d ,       # pv_d - Political Stability and Absence of Violence/Terrorism for country of asylum

           train.p,
           index = c("pair","year"),
           model = "random")

screenreg(list("Modeling variables" = ran.p),digits = 5)
```


Checking errors

```{r  echo=TRUE, message=FALSE, warning=FALSE, comment=""}
test.p$hat1 <- predict(ran.p,newdata = test.p)

test.p <- test.p %>%
          mutate(real = exp(lref),
                 esti1 = exp(hat1))
## check the prediction for all country pairs

testp.error <- test.p %>% 
               transmute(year = year,
                         asylum = asylum,
                         iso_d = iso_d,
                         origin = origin,
                         iso_o = iso_o,
                         pair = pair,
                         real = real, 
                         esti1 = exp(hat1)) %>%
               mutate(error1 = (esti1 - real) / real) %>% 
  as.data.frame()

mean(abs(testp.error$error1))

## check the prediction for all country of asylum

Country_summary_asylum <- testp.error %>%  
              group_by(year,asylum) %>% 
              dplyr::summarise(real = sum(real),
                        esti1 = round(sum(esti1))) %>%
              mutate(error1 = (esti1 - real) / real)

mean(abs(Country_summary_asylum$error1))
summary(abs(Country_summary_asylum$error1))

```

Comparing with the official refugee data of 2018, the accuracy of the random effect model is 80%. 

## Comparing with other data sources

In the above models, we used the Political Stability and Absence of Violence/Terrorism as the push factors of refugee flow. However, there are other alternative resources could be essential for predicting refugee populations, such as
ACLED, "Armed Conflict Location & Event Data Project (ACLED); UCDP - Uppsala Conflict Data Program; INFORM - Index for Risk Management. We selected key variables from those datasets and conducted a comparison with PSAV to decide the best one for the model.

The datasets used in the model could be obtained from

###  ACLED
For [ACLED}(https://acleddata.com/download/13984/), we used the total Fatalities (logarithm) of country-year data. However, one of the drawbacks of ACLED dataset is the years of coverage varies across many countries and regions. FOr instance, the coverage for Europe countries is 1/2018-Presen, while in Africa countries there are data available from 1/1997-Present. Also, due to the uneven coverage, we used the ACLED data only for country of origin in our model. 


```{r  echo=TRUE, message=FALSE, warning=FALSE, comment=""}
## https://www.acleddata.com/download/2833/ 

url <- "https://acleddata.com/download/13984"
destfile = "data-raw/number_of_reported_fatalities_by_country-year_as-of-02May2020.xlsx"
if (!file.exists(destfile)) {
  #setInternet2(TRUE)
  download.file(url ,destfile, method = "curl", quiet = FALSE) }

acled <- read_xlsx("data-raw/number_of_reported_fatalities_by_country-year_as-of-02May2020.xlsx",sheet = "Sheet 1")
#acled_code <- merge(x = acled , by.x = "Country", all.x = TRUE, y = code_o , by.y = "country" )
#as.character(unique(acled_code[which(is.na(acled_code$iso_o)),"Country"]))

acled$Country[acled$Country == "Bolivia"] <- "Bolivia (Plurinational State of)"
acled$Country[acled$Country == "Caribbean Netherlands"] <- "Bonaire, Sint Eustatius and Saba"
acled$Country[acled$Country == "Curacao"] <- "CuraÃ§ao"
acled$Country[acled$Country == "Central African Republic"] <- "Central African Rep."
acled$Country[acled$Country == "Dominican Republic"] <- "Dominican Rep."
acled$Country[acled$Country == "Democratic Republic of Congo"] <- "Dem. Rep. of the Congo"
acled$Country[acled$Country == "eSwatini"] <- "Swaziland"
acled$Country[acled$Country == "Iran"] <- "Iran (Islamic Rep. of)"
acled$Country[acled$Country == "Ivory Coast"] <- "CÃ´te d'Ivoire"
acled$Country[acled$Country == "Kosovo"] <- "Serbia and Kosovo (S/RES/1244 (1999))"
acled$Country[acled$Country == "Serbia"] <- "Serbia and Kosovo (S/RES/1244 (1999))"
acled$Country[acled$Country == "Laos"] <- "Lao People's Dem. Rep."
acled$Country[acled$Country == "Moldova"] <- "Rep. of Moldova"
acled$Country[acled$Country == "North Macedonia"] <- "The former Yugoslav Republic of Macedonia"
acled$Country[acled$Country == "Palestine"] <- "State of Palestine"
acled$Country[acled$Country == "Republic of Congo"] <- "Congo"
acled$Country[acled$Country == "Russia"] <- "Russian Federation"
acled$Country[acled$Country == "Sint Maarten"] <- "Sint Maarten (Dutch part)"
acled$Country[acled$Country == "Saint Vincent and Grenadines"] <- "Saint Vincent and the Grenadines"
acled$Country[acled$Country == "Syria"] <- "Syrian Arab Rep."
acled$Country[acled$Country == "Tanzania"] <- "United Rep. of Tanzania"
acled$Country[acled$Country == "Venezuela"] <- "Venezuela (Bolivarian Republic of)"
acled$Country[acled$Country == "Vietnam"] <- "Viet Nam"
acled$Country[acled$Country == "Virgin Islands, U.S."] <- "British Virgin Islands"

acled_code <- merge(x = acled , by.x = "Country", all.x = TRUE, y = code_o , by.y = "country" )
# as.character(unique(acled_code[which(is.na(acled_code$iso_o)),"Country"]))

acled_data <- acled_code %>% 
  filter(!is.na(iso_o)) %>% 
  group_by(Year,iso_o) %>% 
  dplyr::summarise(Fatalities = sum(Fatalities)) %>% 
  dplyr::rename(year = Year,
         Fatalities_acled = Fatalities)
```


### UCDP  

The [UCDP](http://ucdp.uu.se/downloads/ged/ged191-csv.zip), whose data could go back as far as 1948, has much better coverage. Similarly, we chose the Death_civillians (logarithm) in conflicts as the key variable, and again, only for county of origin.

```{r  echo=TRUE, message=FALSE, warning=FALSE, comment=""}

url <- paste( 'http://ucdp.uu.se/downloads/ged/ged191-csv.zip') 
destfile = "data-raw/ucdp.zip" 
if (!file.exists(destfile)) {
  #setInternet2(TRUE)
  download.file(url ,destfile,method = "auto")
  unzip(destfile,exdir = getwd())  
}
uppsala <- read.csv("data-raw/ged191.csv")

uppsala <- uppsala %>% select(year, country, deaths_civilians)

#uppsala_code <- merge(x = uppsala , by.x = "country", all.x = TRUE, y = code_o , by.y = "country" )

#as.character(unique(uppsala_code[which(is.na(uppsala_code$iso_o)),"country"]))

uppsala$country <- as.character(uppsala$country)
uppsala$country[uppsala$country == "Bolivia"] <- "Bolivia (Plurinational State of)"
uppsala$country[uppsala$country == "Bosnia-Herzegovina"] <- "Bosnia and Herzegovina"
uppsala$country[uppsala$country == "Cambodia (Kampuchea)"] <- "Cambodia"
uppsala$country[uppsala$country == "Central African Republic"] <- "Central African Rep."
uppsala$country[uppsala$country == "DR Congo (Zaire)"] <- "Dem. Rep. of the Congo"
uppsala$country[uppsala$country == "Iran"] <- "Iran (Islamic Rep. of)"
uppsala$country[uppsala$country == "Ivory Coast"] <- "CÃ´te d'Ivoire"
uppsala$country[uppsala$country == "Kingdom of eSwatini (Swaziland)"] <- "Swaziland"
uppsala$country[uppsala$country == "Laos"] <- "Lao People's Dem. Rep."
uppsala$country[uppsala$country == "Macedonia, FYR"] <- "The former Yugoslav Republic of Macedonia"
uppsala$country[uppsala$country == "Madagascar (Malagasy)"] <- "Madagascar"
uppsala$country[uppsala$country == "Rumania"] <- "Romania"
uppsala$country[uppsala$country == "Myanmar (Burma)"] <- "Myanmar"
uppsala$country[uppsala$country == "Moldova"] <- "Rep. of Moldova"
uppsala$country[uppsala$country == "Russia (Soviet Union)"] <- "Russian Federation"
uppsala$country[uppsala$country == "Serbia (Yugoslavia)"] <- "Serbia and Kosovo (S/RES/1244 (1999))"
uppsala$country[uppsala$country == "Tanzania"] <- "United Rep. of Tanzania"
uppsala$country[uppsala$country == "Venezuela"] <- "Venezuela (Bolivarian Republic of)"
uppsala$country[uppsala$country == "Yemen (North Yemen)"] <- "Yemen"
uppsala$country[uppsala$country == "Zimbabwe (Rhodesia)"] <- "Zimbabwe"

uppsala_code <- merge(x = uppsala , by.x = "country", all.x = TRUE, y = code_o , by.y = "country" )

# as.character(unique(uppsala_code[which(is.na(uppsala_code$iso_o)),"country"]))

uppsala_data <- uppsala_code %>%
  select(-country) %>% 
  group_by(year,iso_o) %>% 
  dplyr::summarise(deaths_civilians = sum(deaths_civilians, na.rm = T))

```

### INFORM

The [INFORM](https://drmkc.jrc.ec.europa.eu/inform-index/Portals/0/InfoRM/2020/INFORM2020_TREND_2010_2019_v040_ALL_2.xlsx?ver=2020-02-07-143323-093) is a global index since 2011 with a score between 0 and 10. However, we only used one component of the index - the Hazard and Exposure. The other two components, 'Vulnerability' and 'Lack of Coping Capacities' are duplicated with other predictors in our model, such as 'uprooted people' (include refugees, asylum-seekers and IDP), social-economic level etc. The Hazard and Exposure is a compound index generated from natural and human hazards. The natural hazards include earthquake, tsunami, flood, tropical cyclone and drought, while the human hazards consist of 'current conflict intensity' and 'projected conflict risk'. It integrated all the related data and could be applied for both country of origin and country of asylum.


```{r  echo=TRUE, message=FALSE, warning=FALSE, comment=""}

url <- paste( 'https://drmkc.jrc.ec.europa.eu/inform-index/Portals/0/InfoRM/2020/INFORM2020_TREND_2010_2019_v040_ALL_2.xlsx?ver=2020-02-07-143323-093')
destfile = "data-raw/INFORM2020_TREND_2010_2019_v040_ALL_2.xlsx"
if (!file.exists(destfile)) {
  #setInternet2(TRUE)
  download.file(url ,destfile,method = "auto") }

# inform
inform <- read_xlsx("data-raw/INFORM2020_TREND_2010_2019_v040_ALL_2.xlsx",
                    sheet = "INFORM2020_TREND_2010_2019_v040")

inform_o <- inform %>% select(Iso3, IndicatorName, IndicatorScore, INFORMYear) %>% 
  filter(IndicatorName == "Hazard & Exposure Index") %>% 
  dplyr::rename(iso_o = Iso3,
         HEI_o = IndicatorScore,
         year = INFORMYear) %>% 
  select(-IndicatorName)

inform_d <- inform_o %>% dplyr::rename(iso_d = iso_o, HEI_d = HEI_o)
```


Merging data and creating models

```{r  echo=TRUE, message=FALSE, warning=FALSE, comment=""}
finaldata_acled <- left_join(ref_grav, acled_data, by = c("year", "iso_o"))
finaldata_ucdp <- left_join(finaldata_acled, uppsala_data, by = c("year", "iso_o"))
finaldata_inform <- left_join(finaldata_ucdp, inform_o, by = c("year", "iso_o"))
finaldata_compare <- left_join(finaldata_inform, inform_d, by = c("year", "iso_d"))

finaldata_compare <- finaldata_compare %>% 
  mutate(lfatalities = log(Fatalities_acled),
         ldeath = log(deaths_civilians))

finaldata_compare$lfatalities[finaldata_compare$lfatalities == -Inf ] <- 0

finaldata_compare$ldeath[finaldata_compare$ldeath == -Inf ] <- 0

test.c <- finaldata_compare %>% 
          filter(year == 2018)

train.c <- finaldata_compare %>% 
           filter(year < 2018)

ran.acled <- plm(lref ~ 
               lag.lref +  # lag.lref - log of refugee in last year
               lndist +    # lndist - log of distance
               contig +    # contig - contiguous border or not
               comlang +   # conlang - common language
               colony +    # colony - colonial relationship in history
               comcur +    # comcur - common currency
               comrelig +  # comrelig - common religion
               lpop_o +    # lpop_o - log of population on country of origin
               lpop_d +    # lpop_d - log of population on country of asylum
               lgdp_o +    # lgdp_o - log of GDP on country of origin
               lgdp_d +    # lgdp_d - log of GDP on country of asylum
               lfatalities,  # lfatalities - log of fatalities on country of origin (ACLED)
           train.c,
           index = c("pair","year"),
           model = "random")

ran.ucdp <- plm(lref ~ 
               lag.lref +  # lag.lref - log of refugee in last year
               lndist +    # lndist - log of distance
               contig +    # contig - contiguous border or not
               comlang +   # conlang - common language
               colony +    # colony - colonial relationship in history
               comcur +    # comcur - common currency
               comrelig +  # comrelig - common religion
               lpop_o +    # lpop_o - log of population on country of origin
               lpop_d +    # lpop_d - log of population on country of asylum
               lgdp_o +    # lgdp_o - log of GDP on country of origin
               lgdp_d +    # lgdp_d - log of GDP on country of asylum
               ldeath,     # ldeath - log of deaths_civilians on country of origin (UCDP)

           train.c,
           index = c("pair","year"),
           model = "random")

ran.inform <- plm(lref ~ 
               lag.lref +  # lag.lref - log of refugee in last year
               lndist +    # lndist - log of distance
               contig +    # contig - contiguous border or not
               comlang +   # conlang - common language
               colony +    # colony - colonial relationship in history
               comcur +    # comcur - common currency
               comrelig +  # comrelig - common religion
               lpop_o +    # lpop_o - log of population on country of origin
               lpop_d +    # lpop_d - log of population on country of asylum
               lgdp_o +    # lgdp_o - log of GDP on country of origin
               lgdp_d +    # lgdp_d - log of GDP on country of asylum
               HEI_o +     # HEI_o - Hazard and Exposure on country of origin (INFORM)
               HEI_d,      # HEI_d - Hazard and Exposure on country of asylum (INFORM)
               
           train.c,
           index = c("pair","year"),
           model = "random")

screenreg(list("ACLED" = ran.acled,
               "UCDP" = ran.ucdp,
               "INFORM" = ran.inform,
               "PSAV" = ran.p),
          digits = 5)
```


Checking errors

```{r  echo=TRUE, message=FALSE, warning=FALSE, comment=""}
test.acled <- test.c %>% select(year,origin,iso_o,asylum,iso_d,pair,
                                      lref,lag.lref,lndist,contig,comlang,
                                      colony,comcur,comrelig,
                                      lpop_o,lpop_d,
                                      lgdp_o,lgdp_d,
                                      lfatalities) %>% na.omit()

test.acled$hat_acled <- predict(ran.acled, newdata = test.acled)

test.ucdp <- test.c %>% select(year,origin,iso_o,asylum,iso_d,pair,
                                      lref,lag.lref,lndist,contig,comlang,
                                      colony,comcur,comrelig,
                                      lpop_o,lpop_d,
                                      lgdp_o,lgdp_d,
                                      ldeath) %>% na.omit()

test.ucdp$hat_ucdp <- predict(ran.ucdp, newdata = test.ucdp)

test.inform <- test.c %>% select(year,origin,iso_o,asylum,iso_d,pair,
                                      lref,lag.lref,lndist,contig,comlang,
                                      colony,comcur,comrelig,
                                      lpop_o,lpop_d,
                                      lgdp_o,lgdp_d,
                                      HEI_o,HEI_d) %>% na.omit()

test.inform$hat_inform <- predict(ran.inform, newdata = test.inform)

test.psav <- test.c %>% na.omit()

test.psav$hat_psav <- predict(ran.p, newdata = test.psav)

test.acled <- test.acled %>% 
   select(year, asylum, iso_d, origin, iso_o, pair, lref, hat_acled) %>% 
   mutate(real = exp(lref),
          esti_acled = exp(hat_acled)) %>% 
   mutate(error_acled = (esti_acled - real) / real)

test.ucdp <- test.ucdp %>% 
   select(year, asylum, iso_d, origin, iso_o, pair, lref, hat_ucdp) %>% 
   mutate(real = exp(lref),
          esti_ucdp = exp(hat_ucdp)) %>% 
   mutate(error_ucdp = (esti_ucdp - real) / real)

test.inform <- test.inform %>% 
   select(year, asylum, iso_d, origin, iso_o, pair, lref, hat_inform) %>% 
   mutate(real = exp(lref),
          esti_inform = exp(hat_inform)) %>% 
   mutate(error_inform = (esti_inform - real) / real)

test.psav <- test.psav %>% 
   select(year, asylum, iso_d, origin, iso_o, pair, lref, hat_psav) %>% 
   mutate(real = exp(lref),
          esti_psav = exp(hat_psav)) %>% 
   mutate(error_psav = (esti_psav - real) / real)

```

Summary of predictive capacity.

```{r  echo=TRUE, message=FALSE, warning=FALSE, comment="", results = 'asis'}

# results = 'asis' for stargazer

 summary_c <- data.frame("Mean_of_prediction_error" =
                        c(mean(abs(test.acled$error_acled)),
                          mean(abs(test.ucdp$error_ucdp)),
                          mean(abs(test.inform$error_inform)),
                          mean(abs(test.psav$error_psav))),
                        "RMSE_of_the_model" = 
                        c(rmse(test.acled$lref,test.acled$hat_acled),
                          rmse(test.ucdp$lref,test.ucdp$hat_ucdp),
                          rmse(test.inform$lref,test.inform$hat_inform),
                          rmse(test.psav$lref,test.psav$hat_psav)),
                        "Number_of_Observations" =
                        c(length(ran.acled$residuals),
                          length(ran.ucdp$residuals),
                          length(ran.inform$residuals),
                          length(ran.p$residuals)))
 
rownames(summary_c) <- c("ACLED", "UCDP", "INFORM", "PSAV")

stargazer(summary_c[,], 
          summary = F,  type = 'html', header = F,
          rownames = T,
          title = "Summary of the 4 models - ACLED / UCDP / INFORM / PSAV")
```

From the summary table, we can notice the ACLED data has the highest accuracy in the counties where data is available. Though as we are aimining at predicting total population for refugees, sufficient data coverage is essential. Following this check we retain the PSAV (Political Stability and Absence of Violence/Terrorism) datset as the preferred option to reflect this componnent in our model.

## Visualising Output

### Output by country

Country shows the predicting results of the model in 2018, from any country of origin to country of asylum. Overall values of refugee and estimation, together with average of absolute error, are shown and aligning with the chosen countries.

```{r  echo=TRUE, message=FALSE, warning=FALSE, comment=""}

result  <- testp.error %>% 
  mutate(esti1 = round(esti1),
         error1 = round(error1,3)) %>% 
  select(year,origin,iso_o,asylum,iso_d,esti1,real,error1) %>% 
  dplyr::rename(Year = year, 
         Origin = origin,
         Asylum = asylum,
         Estimation = esti1,
         Refugee = real,
         Error = error1)



DT::datatable({result }, options = list(pageLength = 10)  )

```

### Output by Region
Region sums up the results of the model in 2018 and 2019 at regional level.

```{r }


origin <- reference %>% 
  select(iso_3, 
         UNHCRBureau) %>% 
  dplyr::rename(iso_o = iso_3,
         origin = UNHCRBureau)
origin$origin <- paste0("From-",origin$origin)

asylum <- reference %>% 
  select(iso_3, UNHCRBureau) %>% 
  dplyr::rename(iso_d = iso_3,
         asylum = UNHCRBureau)
asylum$asylum <- paste0("To-",asylum$asylum)

region_origin <- left_join(result, origin, by = "iso_o")

region_bureau <- left_join(region_origin, asylum, by = "iso_d")

region <- region_bureau %>% 
  group_by(Year,origin,asylum) %>% 
  dplyr::summarise( Estimation = sum(Estimation),
             Refugee = sum(Refugee)) %>%
  mutate(Error = round((Estimation - Refugee) / Refugee,3)) %>% 
  arrange(Year)

DT::datatable({region },  options = list(pageLength = 10) )

```

  

### Flow Viz with Sankey Diagram

Region Viz displays the regional results in 2019 with a sankey chart, which shows the refugee movements are mainly within the regions, except for the flow from MENA to Europe.

Region Crossing displays the region-crossing results in 2019, that is - removing displacement within the regions. 

```{r echo=TRUE, fig.height=9, fig.width=9, message=FALSE, warning=FALSE}
### Compile results per bureau to create Sankey Diagramme

# Make a connection data frame
links <- data.frame(
  source = region$origin, 
  target = region$asylum, 
  value = region$Estimation
)
 
# From these flows we need to create a node data frame: it lists every entities involved in the flow
nodes <- data.frame(
  name = c(as.character(links$source), as.character(links$target)) %>% 
    unique()
)

# With networkD3, connection must be provided using id, not using real name like in the links dataframe.. So we need to reformat it.
links$IDsource <- match(links$source, nodes$name) - 1 
links$IDtarget <- match(links$target, nodes$name) - 1

links$group <- as.factor(links$source)
levels(links$group)
nodes$group <- as.factor(c("my_unique_group"))

colorsc <- 'd3.scaleOrdinal() 
                .domain(["From-Americas",    
                         "From-Asia",
                         "From-EastAfrica",  
                         "From-Europe",      
                         "From-MENA",        
                         "From-SouthAfrica",
                         "From-WestAfrica",
                         "my_unique_group"]) 
                .range(["#74879F", "#338EC9", "#18375F","#FCF57F","#66D1C1","#F592A0","#F5C205","#CCCCCC"])'

networkD3::sankeyNetwork( Links = links,
                          Nodes = nodes,
                          Source = "IDsource",
                          Target = "IDtarget",
                          Value = "value",
                          NodeID = "name",
                          fontSize = 11,
                          nodeWidth = 5,
                          fontFamily = "Lato",
                          iterations = 0,
                          #colourScale = colorsc,
                          LinkGroup = "group",
                          NodeGroup = "group")



```

# Prediction for longer time
When we want to predict future, like refugees in 2019, 2020 or 2021, we need to update the time-variant variables, such as refugee in last year, GDP, population and political stability. Here we start with 2019 to illustrate how to build the input dataset. After extracting the refugee in 2018 from the 2018 dataset and renaming as 'lag.lref', we download the GDP data from World Economic Outlook Databases (WEO) [https://www.imf.org/external/pubs/ft/weo/2020/01/weodata/index.aspx] and population data from World Population Prospects[https://population.un.org/wpp/], as the world bank data is only updated to 2018. We cleaned the data and connected it with the time-invariant gravity variables. 

We have no alternative sources for the PSAV index but have to forecast the value of PSAV in 2019 for every country. We transformed the PSAV data into time-series start from 2000, based on which we obtained the PSAV for 2019-2023 with batch prediction.

The input dataset for 2020 could be built in a similar way. Continuing with the iterative game, we could achieve a long-term prediction. However, the error is enlarged simultaneously as a result of the iterative process.

## predict 2019

 Batch for PSAV index


```{r echo=TRUE, message=FALSE, warning=FALSE}

test2019 <- ref_grav %>% filter(year == 2019) %>% 
  select( -pv_o, -pv_d) %>% na.omit()

## batch prediction
PSAV <- wb_data %>% select(year, iso, pv)

## convert the data into time series data

PSAV.wide =  reshape(data = PSAV,
                     idvar= "year",
                     v.names= c("pv"),
                     timevar= "iso",
                     direction = "wide")


ts.PSAV <- ts(PSAV.wide[,-1], f=1 ,s=2000)

# predict the PSAV for next 5 years

ns <- ncol(ts.PSAV)
h <- 5
PSAV.fcast <- matrix(NA, nrow=h, ncol=ns)
colnames(PSAV.fcast) <- colnames(ts.PSAV)

for(i in 1:ns)
  PSAV.fcast[,i] <- forecast(ts.PSAV[,i],h=h)$mean

## reshape the results for connection

PSAV.estimate <- data.frame("year" = c(2019:2023), PSAV.fcast)

PSAV.long <- melt(PSAV.estimate, id.vars = "year")

PSAV.long$variable <- str_sub(PSAV.long$variable,4)

PSAV2019_o <- PSAV.long %>% 
  filter(year == 2019) %>%
  dplyr::rename(iso_o = variable,
         pv_o = value) %>% 
  select(-year)

PSAV2019_d <- PSAV.long %>% 
  filter(year == 2019) %>%
  dplyr::rename(iso_d = variable,
         pv_d = value) %>% 
    select(-year)

## connect to the test dataset

test2019_psav <- left_join(test2019, PSAV2019_o, by = "iso_o") 
  
test2019_final <- left_join(test2019_psav, PSAV2019_d, by = "iso_d")

## predict 2019

test2019_final$hat1 <- predict(ran.p,newdata = test2019_final)

prediction_2019 <- test2019_final %>% 
  mutate(esti1 = round(exp(hat1))) %>% 
  mutate(error1 = round((esti1 - refugee)/refugee,3)) %>% 
  select(year,origin,iso_o,asylum,iso_d,esti1,refugee,error1) %>% 
  dplyr::rename(Year = year, 
         Origin = origin,
         Asylum = asylum,
         Estimation = esti1,
         Refugee = refugee,
         Error = error1)


DT::datatable({prediction_2019 }, options = list(pageLength = 10)  )

```

## predict 2020

The above predicted PSAV Index for predicting refugees in 2020. The population data is avaialble at World Population Prospects[https://population.un.org/wpp/]. However, the GDP data is complicated. The data provided by World Economic Outlook Databases (WEO) [https://www.imf.org/external/pubs/ft/weo/2020/01/weodata/index.aspx] is the GDP at current price, while the GDP data from World Bank, we used in the model, is at constant price. The GDP at constant price in WEO is only available at Percent change. Thus, we need to do the calculation for GDP in 2020 based on the WB data and WEO data.

There is a format matching issue with downloading the WEO data. Here I resaved the WEO data into an excel file.


```{r echo=TRUE, message=FALSE, warning=FALSE}

# dataset for 2020

test2020 <- test2019_final %>% na.omit() %>% 
  select(-refugee,-lref,-pop_d,-pop_o,-lgdp_d,-lgdp_o,-lpop_o,-lpop_d,-pv_o,-pv_d,-lag.lref) %>% 
  dplyr::rename(lag.lref = hat1,
                gdp_o_2019 = gdp_o,
                gdp_d_2019 = gdp_d)

test2020$year <- 2020

PSAV2020_o <- PSAV.long %>% 
  filter(year == 2020) %>%
  dplyr::rename(iso_o = variable,
         pv_o = value) %>% 
  select(-year)

PSAV2020_d <- PSAV.long %>% 
  filter(year == 2020) %>%
  dplyr::rename(iso_d = variable,
         pv_d = value) %>% 
    select(-year)

test2020_psav_o <- left_join(test2020, PSAV2020_o, by = "iso_o") 
  
test2020_psav <- left_join(test2020_psav_o, PSAV2020_d, by = "iso_d")


# gdp data from World Economic Outlook Databases (WEO)

# url <- paste( 'https://www.imf.org/external/pubs/ft/weo/2020/01/weodata/weoreptc.aspx?sy=2020&ey=2021&scsm=1&sic=1&sort=country&ds=.&br=0&pr1.x=43&pr1.y=12&c=512%2C668%2C914%2C672%2C612%2C946%2C614%2C137%2C311%2C546%2C213%2C674%2C911%2C676%2C314%2C548%2C193%2C556%2C122%2C678%2C912%2C181%2C313%2C867%2C419%2C682%2C513%2C684%2C316%2C273%2C913%2C868%2C124%2C921%2C339%2C948%2C638%2C943%2C514%2C686%2C218%2C688%2C963%2C518%2C616%2C728%2C223%2C836%2C516%2C558%2C918%2C138%2C748%2C196%2C618%2C278%2C624%2C692%2C522%2C694%2C622%2C962%2C156%2C142%2C626%2C449%2C628%2C564%2C228%2C565%2C924%2C283%2C233%2C853%2C632%2C288%2C636%2C293%2C634%2C566%2C238%2C964%2C662%2C182%2C960%2C359%2C423%2C453%2C935%2C968%2C128%2C922%2C611%2C714%2C321%2C862%2C243%2C135%2C248%2C716%2C469%2C456%2C253%2C722%2C642%2C942%2C643%2C718%2C939%2C724%2C734%2C576%2C644%2C936%2C819%2C961%2C172%2C813%2C132%2C726%2C646%2C199%2C648%2C733%2C915%2C184%2C134%2C524%2C652%2C361%2C174%2C362%2C328%2C364%2C258%2C732%2C656%2C366%2C654%2C144%2C336%2C146%2C263%2C463%2C268%2C528%2C532%2C923%2C944%2C738%2C176%2C578%2C534%2C537%2C536%2C742%2C429%2C866%2C433%2C369%2C178%2C744%2C436%2C186%2C136%2C925%2C343%2C869%2C158%2C746%2C439%2C926%2C916%2C466%2C664%2C112%2C826%2C111%2C542%2C298%2C967%2C927%2C443%2C846%2C917%2C299%2C544%2C582%2C941%2C474%2C446%2C754%2C666%2C698&s=NGDP_RPCH&grp=0&a=')
# 
#  destfile = "data-raw/WEO_Data.xls"
#  imf <- read_excel("data-raw/WEO_Data.xls",sheet = "WEO_Data")


#  imf <- read_xls("data-raw/WEO_Data.xls",sheet = "WEO_Data")
# 
imf <-  unhcrdatapackage::GDP_IMF

gdp_imf <- imf %>% 
  select(ISO,`2020`,`2021`) %>% 
  dplyr::rename(
         gdp2020 = `2020`,
         gdp2021 = `2021`)

gdp_imf$gdp2020[gdp_imf$gdp2020 == "n/a"] <- NA
gdp_imf$gdp2021[gdp_imf$gdp2021 == "n/a"] <- NA


gdp_imf_panel = reshape( data = gdp_imf,
                         idvar= "ISO",
                         varying = 2:3,
                         sep= "",
                         timevar= "year",
                         times = c(2020,2021),
                         new.row.names= 1:1000,
                         direction = "long")

gdp_imf_panel$gdp <- as.numeric(gdp_imf_panel$gdp)

gdp2020_o <- gdp_imf_panel %>% 
  filter(year == 2020) %>% 
  dplyr::rename(iso_o = ISO,
         gdp_o_pc = gdp) %>% 
  select(-year)

gdp2020_d <- gdp_imf_panel %>% 
  filter(year == 2020) %>% 
  dplyr::rename(iso_d = ISO,
         gdp_d_pc = gdp) %>% 
  select(-year)

test2020_gdp_o <- left_join(test2020_psav, gdp2020_o, by = "iso_o") 
  
test2020_gdp <- left_join(test2020_gdp_o, gdp2020_d, by = "iso_d")  

test2020_gdp <- test2020_gdp %>% 
  dplyr::mutate(gdp_o = gdp_o_2019 * ( 1 + gdp_o_pc),
         gdp_d = gdp_d_2019 * ( 1 + gdp_d_pc))

# population data from world population prospects


url <- "https://population.un.org/wpp/Download/Files/1_Indicators%20(Standard)/EXCEL_FILES/1_Population/WPP2019_POP_F01_1_TOTAL_POPULATION_BOTH_SEXES.xlsx"
destfile = "data-raw/WPP2019_POP_F01_1_TOTAL_POPULATION_BOTH_SEXES.xlsx"
if (!file.exists(destfile)) {
  #setInternet2(TRUE)
  download.file(url ,destfile, method = "curl", quiet = FALSE) }

wpp<- read_xlsx("data-raw/WPP2019_POP_F01_1_TOTAL_POPULATION_BOTH_SEXES.xlsx",sheet = "ESTIMATES")
wpp <- wpp[-(1:11),]
colnames(wpp) <- wpp[1,]
wpp <- wpp[-1,]

pop2020 <- wpp %>% select(`Country code`,`2020`) %>% 
  dplyr::rename(M49_code = `Country code`,
         pop = `2020`)

pop2020$M49_code <- as.numeric(pop2020$M49_code)
pop2020$pop <- as.numeric(pop2020$pop)

code_o_wpp <- reference %>% 
  select(iso_3,M49_code) %>% 
  dplyr::rename(iso_o = iso_3)
code_d_wpp <- reference %>% 
  select(iso_3,M49_code) %>% 
  dplyr::rename(iso_d = iso_3)




pop2020_o <- left_join(pop2020, code_o_wpp, by = "M49_code")
pop2020_o <- pop2020_o %>% 
  select(iso_o, pop) %>%
  dplyr::rename(pop_o = pop) %>% 
  na.omit()

pop2020_d <- left_join(pop2020, code_d_wpp, by = "M49_code")
pop2020_d <- pop2020_d %>% 
  select(iso_d, pop) %>%
  dplyr::rename(pop_d = pop) %>% 
  na.omit()


test2020_pop_o <- left_join(test2020_gdp, pop2020_o, by = "iso_o") 
  
test2020_pop <- left_join(test2020_pop_o, pop2020_d, by = "iso_d")  


test2020_final <- test2020_pop %>% 
  mutate(lpop_o = log(pop_o),
                     lpop_d = log(pop_d),
                     lgdp_o = log(gdp_o),
                     lgdp_d = log(gdp_d)) %>% 
  na.omit()

test2020_final$hat1 <- predict(ran.p,newdata = test2020_final)

prediction_2020 <- test2020_final %>% 
                   mutate(esti1 = round(exp(hat1))) %>%
                   select(year,
                          origin,
                          iso_o,
                          asylum,
                          iso_d,
                          pair,esti1) %>% 
                  dplyr::rename(pred = esti1)


table2020 <- prediction_2020 %>% 
  select(year,origin,iso_o,asylum,iso_d,pred) %>% 
  dplyr::rename(Year = year, 
         Origin = origin,
         Asylum = asylum,
         Estimation = pred)
DT::datatable({table2020 }, options = list(pageLength = 10) )

```

## 